{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9efd1a19",
   "metadata": {},
   "source": [
    "# Uncovering Structure in Data\n",
    "\n",
    "Data can be hard to understand sometimes. We can picture it in 2 dimensions, and maybe 3 dimensions, if we use graphs. But what about in higher dimensions? In this case we might want to view our data in a lower dimension without losing too much information. Or, we want to uncover structure that we suspect might be there. In this case, the structure might be smaller groupings of the data where similar observations are grouped together.\n",
    "\n",
    "In this lesson we'll cover two approaches that can help us better understand our data by uncovering structure. These two methods are:\n",
    "1. Hierarchical Clustering (HCL)\n",
    "    * Given observation described by features, it is common to want similar features to be grouped together.\n",
    "    * Sometimes we have an idea of what the structure should be, but we don't always know how to get there. HCL doesn't require us to specify what the structure is beforehand.\n",
    "2. Principal Component Analysis (PCA)\n",
    "    * Finds a low-dimensional representation that can allow us to view the data and observe structure\n",
    "    * Balances low-dimensional representation without losing too much information\n",
    "    \n",
    "\n",
    "\n",
    "### Note\n",
    "Many of my images will be drawn from this [book](https://people.unica.it/claudioconversano/files/2015/02/ISLR_print4.pdf). I highly recommend reading it if you're interested in more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72415f5b",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Please navigate [here](https://ondemand.osc.edu/pun/sys/dashboard/batch_connect/sessions) and start a **VDI (Owens and Pitzer)** interactive session. Please be sure to use **4 hours** so we can use it the whole session\n",
    "\n",
    "<img src=\"img/VDI_interactive_4hr.png\" alt=\"vdi\" style=\"width: 500px;\"/>\n",
    "\n",
    "Open a terminal by clicking on the terminal icon at the bottom.\n",
    "\n",
    "Then type these commands\n",
    "\n",
    "Move the data and files to your home directory\n",
    "\n",
    "`cp /fs/ess/PAS1956/indigiData_clustering/indigiData_clustering.zip ./`\n",
    "\n",
    "\n",
    "Unzip the data\n",
    "\n",
    "`unzip indigiData_clustering.zip`\n",
    "\n",
    "Move into the directory\n",
    "\n",
    "`cd indigiData_clustering`\n",
    "\n",
    "Create our anaconda environment\n",
    "\n",
    "`conda env create -f environment.yml`\n",
    "\n",
    "Activate the anaconda environment\n",
    "\n",
    "`conda activate indigiData_clustering`\n",
    "\n",
    "Start jupyter notebook\n",
    "\n",
    "`jupyter notebook`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f1cf3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a6b5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14a6616c",
   "metadata": {},
   "source": [
    "## Python programming language\n",
    "Please run this code as soon as you can. Any issues here are usually just install issues, and though annoying can take time to solve.\n",
    "\n",
    "Let's talk a bit about the language we'll be using here today.\n",
    "\n",
    "Python is a programming language like R, in that you can call functions, create variables, automate analyses, iterate through data... lots of stuff!\n",
    "\n",
    "Python is a little bit different though too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1c034",
   "metadata": {},
   "source": [
    "### How to use Jupyte notebook\n",
    "Jupyter notebook allows us to type and execute code. We can type code in these cells, and we can execute or run the code by clicking on a cell and pressing `shift + enter`. \n",
    "\n",
    "You can add more cells by using the `+ symbol` in the top left of the screen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfb4131",
   "metadata": {},
   "source": [
    "### Creating variables with data types in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a42b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1ac58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f88722dd",
   "metadata": {},
   "source": [
    "### Built-in functions in python\n",
    "These are function that come with the standard python environment. You can also use functions that are imported from other libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df69fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6149e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "059c3b89",
   "metadata": {},
   "source": [
    "### User defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81664d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49c5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5977a964",
   "metadata": {},
   "source": [
    "## Libraries Needed\n",
    "Please run this code as soon as you can. Any issues here are usually just install issues, and though annoying can take time to solve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ade3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.colors import ListedColormap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c3630",
   "metadata": {},
   "source": [
    "## Features\n",
    "What do I mean when I talk about observation being described by features? Here, I mean that an observation is represented by a list or series of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af881147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9af70cf2",
   "metadata": {},
   "source": [
    "The above example is an observation with 6 features, ie 6 dimensions.\n",
    "\n",
    "If we stack these observations on top of each other we'd have a list of lists, or a matrix, where each observation is represented by a row, and each row has 6 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20607bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23c191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bdbc64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3010322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4adec77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44754759",
   "metadata": {},
   "source": [
    "But what do these features mean? Well, that depends on our data.\n",
    "\n",
    "1. Text data\n",
    "    * Each observation could be a document.\n",
    "    * Each element in our feature list for a document might be a whole number, representing he number of times a word appears in that document.\n",
    "2. Disease codes\n",
    "    * Each observation is a patient.\n",
    "    * Each element in our feature list might be the presence of a disease code, the value for a labratory test, a vital sign value, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a73fbe",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "Hierarchical clustering is an appraoch the builds an upside down tree of relationships between observations. This is known as a dendrogram.\n",
    "\n",
    "That's a little bit confusing. Sometimes an example can help!\n",
    "Below is an example of 2 dimensional data in a flat representation. What might this look like as a dendrogram?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d117763b",
   "metadata": {},
   "source": [
    "<img src=\"img/example_2d_data.png\" alt=\"flat_2d_cluster\" style=\"width: 500px;\"/>\n",
    "An example of data points in a flat clustering representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d1bd3",
   "metadata": {},
   "source": [
    "<img src=\"img/dendrogram_1.png\" alt=\"dendrogram_no_cut\" style=\"width: 500px;\"/>\n",
    "The flat data represented as a dendrogram. Notice that there aren't  any clusters just yet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e41d5",
   "metadata": {},
   "source": [
    "So, how do we interpret this dendrogram?\n",
    "* The observations along the bottom are known as leaves. \n",
    "* As we move up, we begin to fuse leaves. These are known as subgroups or clusters.\n",
    "* The earlier these fusions happen the more similar those observations are than leaves which were fused later.\n",
    "    * Be careful with spatial interpretations. Height matters, but distance along the x axis does not. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c7f81",
   "metadata": {},
   "source": [
    "<img src=\"img/dendrogram_cuts_cropped.png\" alt=\"dendrogram_cut\" style=\"width: 2000px;height: 400px;\"/>\n",
    "By choosing the point on a point on the y axis we can choose what clusters we wantto create\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62daa352",
   "metadata": {},
   "source": [
    "One thing you might notice is that there aren't any clusters even though we've induced some kind of structure. In order to identify clusters from the structure we need to choose where to \"cut\" along the y axis.\n",
    "\n",
    "The y axis represents how close observations are, as in the lower on the y axis leaves are fused the more similar they are.\n",
    "\n",
    "Next you might ask how do we measure how similar or close leaves are? How does that apply to groups of leaves?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b1267",
   "metadata": {},
   "source": [
    "### Measures of distance\n",
    "Recall that we're working with observation that are represented as lists of numbers. We can measure how close or similar these observations are to one another using these features.\n",
    "\n",
    "Many distance measurs exist. Examples include:\n",
    "* Euclidean distance\n",
    "    * A measure which looks at the distance between each corresponding feature in two lists of features.\n",
    "    * A standard measure that is used, but can weigh certain features more than others if they are big.\n",
    "* Cosine distance\n",
    "    * Measures the cosine of an angle between two lists of features\n",
    "    * Usually a safe measure, unless you care about how \"big\" feature lists are.\n",
    "    * Technically the distance is 1 - cos(angle between lists of features)\n",
    "* Pearson distance\n",
    "    * This measures the correlation between two points.\n",
    "    * Technically it is 1- the correlation, because a distance metric of 0 means that two points are as close as they can be to one another.\n",
    "    \n",
    "Overall, the distance measure matters a great deal for HCL, and in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab8a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,1], [2,2]])\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edecb285",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f038a",
   "metadata": {},
   "source": [
    "### Exercise: Calculate the distance between the two points using two three different distance measures\n",
    "[pdist](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html#scipy.spatial.distance.pdist) is a good function to calculate distances. Look at the documentation and see how you might use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cbe86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28165ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e638fc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de013b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c3305f1",
   "metadata": {},
   "source": [
    "### Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa4a0c",
   "metadata": {},
   "source": [
    "1. Begin with all $n$ observations and a distance/dissimilarity measure. \n",
    "2. Measure the distance between all observations, treating each observation as its own cluster.\n",
    "3. For $i = n, n-1, n-2, ... 2$:\n",
    "    * (a) Examine all the inter-cluster distances among the $i$ clusters. Fuse the two clusters that are most similar. The distance between these two clusters indicates the height (yaxis) in the dendrogram to place this merge. \n",
    "    * (b) Compute the new pairwise distance among the remaining clusters (including the fused cluster)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a644e05",
   "metadata": {},
   "source": [
    "## How do we measure the distance between clusters?\n",
    "Measuring the distance when each \"cluster\" has only one observation is fairly intuitive. But when these clusters contain multiple observations we have to get a little fancy. Here are examples of how to measure the distance between clusters.\n",
    "\n",
    "* Complete (maximum distance)\n",
    "    * Computer all pairwise distances between the observation in two clusters and take the *largest* distance.\n",
    "* Single (minimum distance)\n",
    "    * Computer all pairwise distances between the observation in two clusters and take the *smallest* distance.\n",
    "* Average\n",
    "    * Computer all pairwise distances between the observation in two clusters and take the *average* distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0977b00",
   "metadata": {},
   "source": [
    "### Coding Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c167937",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c725053",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5066e7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a76ab4d",
   "metadata": {},
   "source": [
    "## Exercise: Explore the data\n",
    "\n",
    "This is left up to you. If we have time I'll go through a couple examples, but here are some things to try.\n",
    "\n",
    "Try clustering the other data files. \n",
    "\n",
    "Try some different distance metrics and cluster distance metrics. Look at the documentation for [scipy.cluster.hierarchy.linkage](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) to see how you can change these arguments. Try reading in different sample data and seeing how the dendrogram changes. **Do you need to perform any transformations on the data we did before?**\n",
    "\n",
    "At the end, think about which distance metric is right to be using?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54fd923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67a7705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d38223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4804b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c42b674b",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "Recall that we needed clustering in the first place because there are times when our data is in too high of a dimension for us to visualize it. Even though we wanted to uncover structure, or confirm structure which should be there, we couldn't without HCL. \n",
    "\n",
    "\n",
    "1. Principal Component Analysis (PCA) can allow us to reduce dimensions down to two or three which allows us to visualize it easier.\n",
    "2. Even if we don't want to visualize the data in lower dimensions, we might want to represent observations with their most important features.\n",
    "3. PCA allows us to reduce dimensionality while maintaining the underlying structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592da3cb",
   "metadata": {},
   "source": [
    "### What is PCA?\n",
    "I'm going to do a lot of handwaving here,  but PCA is attempting to identify the contributions that distinguishes the dataset the best. \n",
    "\n",
    "Let's say, I'm trying to sell you a house. And I give you 9,000 pictures of views through the windows of that house instead of letting you view the house. Many of those windows view very similar settings, maybe with slightly different angles.\n",
    "\n",
    "So you, as a smart buyer, find a combination of the windows by overlaying the pictures on top of each other so you have the most comprehensive view that you can get with one, two, or three images. You also decide to vary the transparency of these pictures when you're combining them.\n",
    "\n",
    "This, is the basic idea behind PCA. We have a lot of data, where each feature is a window. We combine these windows in some way in order to condense the number of windows down to two or three windows. Whatever we want really.\n",
    "\n",
    "\n",
    "When discussing PCA, these windows are often referrred to as new dimensions distinguish the data the best. The way that we find to combine the windows is known as a transformation of the original data. \n",
    "\n",
    "\n",
    "Credit to Audra Crouch for the window idea.\n",
    "\n",
    "\n",
    "Let's interpret this toy example. [source](https://towardsdatascience.com/principal-components-analysis-explained-53f0639b2781)\n",
    "\n",
    "<img src=\"img/PCA_example.png\" alt=\"PCA_interp\" style=\"width: 2000px;height: 400px;\"/>\n",
    "\n",
    "\n",
    "Generally speaking, it's difficult to interpret these directions and is subjective. Once we perform this transformation on the data, the individual features lose their meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df385db",
   "metadata": {},
   "source": [
    "### Coding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041fa20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/All_taxa.csv\"\n",
    "# data_path = \"data/Fungi.csv\"\n",
    "# data_path = \"data/Bacteria.csv\"\n",
    "# data_path = \"data/Protist.csv\"\n",
    "# data_path = \"data/Archaea.csv\"\n",
    "data_df = pd.read_csv(data_path, skiprows=[0])\n",
    "data_df = data_df.transpose()\n",
    "data_df = data_df.rename(columns=data_df.iloc[0])\n",
    "data_df = data_df.drop(labels = \"Location\", axis = \"index\")\n",
    "new_index_names = [\"cattle_0\", \"cattle_1\", \"cattle_2\", \"cattle_3\", \"cattle_4\",\n",
    "                      \"hay_5\", \"hay_6\", \"hay_7\", \"hay_8\", \"hay_9\"]\n",
    "data_df.index = new_index_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931305a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275ec55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 dimensional PCA decomposition\n",
    "y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "colormap = ListedColormap([\"r\",\"b\"])\n",
    "classes = ['Cattle', 'Hay']\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap = colormap)\n",
    "plt.legend(handles=scatter.legend_elements()[0],\n",
    "           labels=classes,\n",
    "           loc='lower right',\n",
    "           ncol=3,\n",
    "           fontsize=12)\n",
    "plt.xlabel(\"principle component 1\")\n",
    "plt.ylabel(\"principle component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48d5f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 dimensional PCA decomposition\n",
    "pca = decomposition.PCA(n_components=3)\n",
    "pca.fit(data_df_std)\n",
    "X = pca.transform(data_df)\n",
    "y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    " \n",
    "# Creating figure\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "ax = plt.axes(projection =\"3d\")\n",
    " \n",
    "# Creating plot\n",
    "ax.scatter3D(X[:, 0], X[:, 1], X[:, 2], c=y, cmap = colormap)\n",
    "plt.title(\"simple 3D scatter plot\")\n",
    " \n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0898bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b9198",
   "metadata": {},
   "source": [
    "### Exercise: Edit some of the plot parameters\n",
    "This is more of an exercise in presentation. Can you change the of the points? Change the titles and labels for the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46c2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758338cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d56d5a54",
   "metadata": {},
   "source": [
    "## Accuracy vs Precision\n",
    "Let's say that we have a quantity that we're trying to measure. Here, the quantity is some measurable phenomenon like the temperature of a glass of water or oxygen content in a patient's blood. We have repeated measuremens of this quantity. Following the previous examples this might be thermometers in the water, or a pulse oximeter. We can define two measurements of good observations.\n",
    "1. Accuracy\n",
    "    * Measures the difference between the average of our observations and the true value of the random variable being measured.\n",
    "    * Accuracy tells us about systematic errors, or errors that come from our ability to measure the quantity well (Imagine a broken thermometer)\n",
    "    * To measure accuracy we would need a reference value. If we knew the true value of the quantity we were tryingto measure we wouldn't be measuring it. A reference value allows us to calibrate the measurement tools.\n",
    "2. Precision\n",
    "    * Measures how close the observations are to each other.\n",
    "    * Precision tells us about random errors, orthe variability of the quantity being measured.\n",
    "    \n",
    "All in all, we can think of accuracy as telling us how far away our measurements are from the true value, and precision as the spread or variability of our data. These two values tell us how good our measurements are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851aadc",
   "metadata": {},
   "source": [
    "### Coding Example\n",
    "Let's say we want to measure a quantity that is centered around the value 5. Let's draw some measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a52c5",
   "metadata": {},
   "source": [
    "### Small example where things go well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = np.random.normal(loc = 5, scale = 0.1, size = 10)\n",
    "print(measurements)\n",
    "print(\"\\nMean: {}\".format(np.mean(measurements)))\n",
    "print(\"\\nStandard deviation: {}\".format(np.std(measurements)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3445095",
   "metadata": {},
   "source": [
    "### Small example where precision is bad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = np.random.normal(loc = 5, scale = 10, size = 10)\n",
    "print(measurements)\n",
    "print(\"\\nMean: {}\".format(np.mean(measurements)))\n",
    "print(\"\\nStandard deviation: {}\".format(np.std(measurements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb7f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = np.random.normal(loc = 5, scale = 10, size = 100)\n",
    "print(\"\\nMean: {}\".format(np.mean(measurements)))\n",
    "print(\"\\nStandard deviation: {}\".format(np.std(measurements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8064659",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = np.random.normal(loc = 5, scale = 10, size = 1000)\n",
    "print(\"\\nMean: {}\".format(np.mean(measurements)))\n",
    "print(\"\\nStandard deviation: {}\".format(np.std(measurements)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaecbeab",
   "metadata": {},
   "source": [
    "### Small example where accuracy is bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fdd026",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = np.random.normal(loc = 5, scale = 10, size = 1000) + 0.1\n",
    "print(\"\\nMean: {}\".format(np.mean(measurements)))\n",
    "print(\"\\nStandard deviation: {}\".format(np.std(measurements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3569706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = np.random.normal(loc = 5, scale = 10, size = 1000) + 2\n",
    "print(\"\\nMean: {}\".format(np.mean(measurements)))\n",
    "print(\"\\nStandard deviation: {}\".format(np.std(measurements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af53f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = np.random.normal(loc = 5, scale = 10, size = 5000) + 2\n",
    "print(\"\\nMean: {}\".format(np.mean(measurements)))\n",
    "print(\"\\nStandard deviation: {}\".format(np.std(measurements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00234029",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = np.random.normal(loc = 5, scale = 10, size = 10000) + 2\n",
    "print(\"\\nMean: {}\".format(np.mean(measurements)))\n",
    "print(\"\\nStandard deviation: {}\".format(np.std(measurements)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb8a48",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "One issue we might run into is randomness or noise in our data. This is a normal occurence in data collection, even with the most strict and best tools. Noise impacts our analysis by questioning whether we can trust our clusters. Maybe we got a few observations that make it look like there is structure when there really isn't. So how can we address this?\n",
    "\n",
    "One way to address noise in your analysis is to use bootstrapping. Bootstrapping is a resampling technique that mimicks the original sampling process and allowing us to generate multiple datasets from the original dataset. In our case, this allows us to generate multiple clusters to assess stability of our cluster assignments.\n",
    "\n",
    "Bootstrapping works by \n",
    "1. Sample with replacement n observations from a dataset of size n\n",
    "2. Calculate your statistic of interest (clustering assignments, mean, variance, etc)\n",
    "3. Repeat h times. h is usually a large number like 500 or 1000 in my experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0368410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(loc = 5, scale = 3, size = 20)\n",
    "print(\"\\nMean: {}\".format(np.mean(X)))\n",
    "print(\"\\nStandard deviation: {}\".format(np.std(X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b6b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap the data\n",
    "\n",
    "mean_list = []\n",
    "stdev_list = []\n",
    "for i in range(5000):\n",
    "    X_bootstrap = np.random.choice(X, size = len(X), replace = True)\n",
    "    mean_list.append(np.mean(X_bootstrap))\n",
    "    stdev_list.append(np.std(X_bootstrap))\n",
    "print(\"\\nbootstrap Mean: {}\".format(np.mean(mean_list)))\n",
    "print(\"\\nbootstrap Standard deviation: {}\".format(np.mean(stdev_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e6570b",
   "metadata": {},
   "source": [
    "So how does this method help withour analysis? Well, take our dendrogram/clustering. Are we sure this is a stable clustering. One way we can test this is by gathering more data, but sometimes that's impossible. We could bootstrap our data and cluster each bootstrapped dataset to see how often we recreate these clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e70086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = data_df.sample(n = 10, replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c4c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = hierarchy.linkage(y = sampled_df, metric = \"euclidean\", method = 'average')\n",
    "plt.figure(figsize=(10, 10))\n",
    "dn = hierarchy.dendrogram(Z,  labels = data_df.index.values, leaf_rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee2f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the bootstrap means\n",
    "\n",
    "n, bins, patches = plt.hist(mean_list, 50, density=False, facecolor='g', alpha=0.75)\n",
    "\n",
    "\n",
    "plt.xlabel('Bootstrapped mean')\n",
    "plt.ylabel('Bin Counts')\n",
    "plt.title('Histogram of bootstrapped means')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad00d3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
